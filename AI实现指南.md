# AIå®ç°æŒ‡å—

## ğŸ¯ åŸºç¡€AIç®—æ³•å®ç°æŒ‡å¯¼

æœ¬æŒ‡å—æä¾›äº†å‡ ç§åŸºç¡€AIç®—æ³•çš„è¯¦ç»†å®ç°æ–¹æ³•ï¼Œé€‚åˆåˆå­¦è€…å¾ªåºæ¸è¿›åœ°å­¦ä¹ æ¸¸æˆAIå¼€å‘ã€‚

## ğŸ“š AIç®—æ³•éš¾åº¦ç­‰çº§

### ğŸŸ¢ å…¥é—¨çº§ï¼ˆæ¨èæ–°æ‰‹ï¼‰
1. **æ”¹è¿›éšæœºAI** - åœ¨éšæœºé€‰æ‹©çš„åŸºç¡€ä¸Šæ·»åŠ åŸºæœ¬è§„åˆ™
2. **åŸºäºè§„åˆ™çš„AI** - ä½¿ç”¨if-elseæ¡ä»¶åˆ¤æ–­çš„ç­–ç•¥AI

### ğŸŸ¡ ä¸­çº§
3. **è´ªå¿ƒç®—æ³•AI** - æ¯æ­¥é€‰æ‹©å½“å‰æœ€ä¼˜åŠ¨ä½œ
4. **ç®€å•æœç´¢AI** - ä½¿ç”¨BFSæˆ–DFSè¿›è¡Œè·¯å¾„æœç´¢

### ğŸ”´ è¿›é˜¶çº§ï¼ˆé€‰åšï¼‰
5. **å¯å‘å¼AI** - ç»“åˆå¤šç§å¯å‘å¼å‡½æ•°
6. **å¼ºåŒ–å­¦ä¹ AI** - ä½¿ç”¨Q-learningç­‰æ–¹æ³•
7. **å¤§è¯­è¨€æ¨¡å‹AI** - æ¥å…¥GPTç­‰å¤§æ¨¡å‹

## ğŸ”§ 1. æ”¹è¿›éšæœºAI



## ğŸ§ª æµ‹è¯•å’Œè°ƒä¼˜

### æ€§èƒ½æµ‹è¯•

```bash
# æµ‹è¯•åŸºç¡€AIæ€§èƒ½
python evaluate_ai.py --agents improved_random rule_based greedy_snake --benchmark --games 100

# æ¯”è¾ƒä¸åŒAI
python evaluate_ai.py --agents random improved_random rule_based --compare --games 50

# ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
python evaluate_ai.py --agents rule_based --benchmark --games 200 --plot --save rule_based_results.json
```

### è°ƒä¼˜å»ºè®®

1. **å‚æ•°è°ƒä¼˜**
   - è°ƒæ•´è¯„ä¼°å‡½æ•°ä¸­å„é¡¹çš„æƒé‡
   - ä¿®æ”¹æœç´¢æ·±åº¦å’Œå®½åº¦
   - ä¼˜åŒ–è§„åˆ™ä¼˜å…ˆçº§

2. **æ€§èƒ½ä¼˜åŒ–**
   - æ·»åŠ ç¼“å­˜é¿å…é‡å¤è®¡ç®—
   - ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„
   - é™åˆ¶æœç´¢æ—¶é—´

3. **ç­–ç•¥æ”¹è¿›**
   - åˆ†æå¤±è´¥çš„æ¸¸æˆæ‰¾å‡ºé—®é¢˜
   - æ·»åŠ æ–°çš„è§„åˆ™æˆ–è¯„ä¼°é¡¹
   - ç»“åˆå¤šç§ç®—æ³•çš„ä¼˜åŠ¿

## ğŸ’¡ å®ç°æŠ€å·§

### 1. è°ƒè¯•æŠ€å·§
```python
def get_action(self, observation, env):
    action = self.calculate_best_action(observation, env)
    
    # è°ƒè¯•è¾“å‡º
    if self.debug:
        print(f"Player {self.player_id} chose action {action}")
        print(f"Board state: {observation['board']}")
    
    return action
```

### 2. å¼‚å¸¸å¤„ç†
```python
def get_action(self, observation, env):
    try:
        return self.smart_action(observation, env)
    except Exception as e:
        print(f"Error in {self.name}: {e}")
        # é™çº§åˆ°ç®€å•ç­–ç•¥
        return random.choice(env.get_valid_actions())
```

### 3. æ—¶é—´æ§åˆ¶
```python
import time

def get_action(self, observation, env):
    start_time = time.time()
    timeout = 5.0  # 5ç§’è¶…æ—¶
    
    for depth in range(1, 10):
        if time.time() - start_time > timeout:
            break
        
        action = self.search_with_depth(observation, env, depth)
    
    return action
```

## ğŸš€ 5. å¼ºåŒ–å­¦ä¹ AIï¼ˆæŒ‘æˆ˜é€‰åšï¼‰

### åŸºæœ¬æ€è·¯
- é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥
- ä½¿ç”¨Q-learningæˆ–ç®€åŒ–çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ 
- éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®å’Œæ—¶é—´

### Q-Learningå®ç°ç¤ºä¾‹

```python
import pickle
import numpy as np

class QLearningBot(BaseAgent):
    def __init__(self, name="QLearningBot", player_id=1):
        super().__init__(name, player_id)
        self.q_table = {}
        self.learning_rate = 0.1
        self.discount_factor = 0.9
        self.epsilon = 0.1  # æ¢ç´¢ç‡
        self.training = True
        
    def get_action(self, observation, env):
        state = self.observation_to_state(observation)
        valid_actions = env.get_valid_actions()
        
        # Îµ-è´ªå¿ƒç­–ç•¥
        if self.training and random.random() < self.epsilon:
            return random.choice(valid_actions)
        
        # é€‰æ‹©Qå€¼æœ€é«˜çš„åŠ¨ä½œ
        q_values = {}
        for action in valid_actions:
            q_values[action] = self.q_table.get((state, action), 0.0)
        
        best_action = max(q_values, key=q_values.get)
        return best_action
    
    def observation_to_state(self, observation):
        """å°†è§‚å¯Ÿè½¬æ¢ä¸ºçŠ¶æ€è¡¨ç¤º"""
        # ç®€åŒ–çŠ¶æ€è¡¨ç¤ºï¼ˆå¯ä»¥æ”¹è¿›ï¼‰
        if 'board' in observation:
            # äº”å­æ£‹ï¼šä½¿ç”¨æ£‹ç›˜çš„ç®€åŒ–è¡¨ç¤º
            board = observation['board']
            return tuple(board.flatten())
        else:
            # è´ªåƒè›‡ï¼šä½¿ç”¨å…³é”®ä¿¡æ¯
            snake = observation.get(f'snake{self.player_id}', [])
            foods = observation.get('foods', [])
            return (tuple(snake[:3]), tuple(foods[:2]))  # ç®€åŒ–è¡¨ç¤º
    
    def update_q_value(self, state, action, reward, next_state):
        """æ›´æ–°Qå€¼"""
        current_q = self.q_table.get((state, action), 0.0)
        
        # è·å–ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
        next_actions = self.get_possible_actions(next_state)
        max_next_q = 0.0
        if next_actions:
            max_next_q = max([self.q_table.get((next_state, a), 0.0) 
                             for a in next_actions])
        
        # Q-learningæ›´æ–°å…¬å¼
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[(state, action)] = new_q
    
    def train(self, env, episodes=1000):
        """è®­ç»ƒQ-learningæ™ºèƒ½ä½“"""
        print(f"å¼€å§‹è®­ç»ƒ {episodes} è½®...")
        
        for episode in range(episodes):
            observation, _ = env.reset()
            state = self.observation_to_state(observation)
            total_reward = 0
            
            while not env.is_terminal():
                action = self.get_action(observation, env)
                next_obs, reward, done, info = env.step(action)
                next_state = self.observation_to_state(next_obs)
                
                # æ›´æ–°Qå€¼
                self.update_q_value(state, action, reward, next_state)
                
                state = next_state
                observation = next_obs
                total_reward += reward
                
                if done:
                    break
            
            # è¡°å‡æ¢ç´¢ç‡
            if episode % 100 == 0:
                self.epsilon = max(0.01, self.epsilon * 0.995)
                print(f"Episode {episode}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.3f}")
    
    def save_model(self, filename):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        with open(filename, 'wb') as f:
            pickle.dump(self.q_table, f)
    
    def load_model(self, filename):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            with open(filename, 'rb') as f:
                self.q_table = pickle.load(f)
            self.training = False
            print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {filename}")
        except FileNotFoundError:
            print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
```

### è®­ç»ƒè„šæœ¬ç¤ºä¾‹

```python
def train_q_learning_bot():
    """è®­ç»ƒQ-learningæ™ºèƒ½ä½“"""
    from games.gomoku import GomokuEnv
    from agents import RandomBot
    
    env = GomokuEnv(board_size=9, win_length=5)
    q_bot = QLearningBot("Q-Learning Bot", 1)
    random_bot = RandomBot("Random Bot", 2)
    
    # è®­ç»ƒé˜¶æ®µ
    q_bot.train(env, episodes=5000)
    
    # ä¿å­˜æ¨¡å‹
    q_bot.save_model("q_learning_gomoku.pkl")
    
    # æµ‹è¯•é˜¶æ®µ
    q_bot.training = False
    q_bot.epsilon = 0  # å…³é—­æ¢ç´¢
    
    # è¯„ä¼°æ€§èƒ½
    wins = 0
    test_games = 100
    for _ in range(test_games):
        observation, _ = env.reset()
        while not env.is_terminal():
            if env.game.current_player == 1:
                action = q_bot.get_action(observation, env)
            else:
                action = random_bot.get_action(observation, env)
            observation, _, done, _ = env.step(action)
            if done:
                break
        
        if env.get_winner() == 1:
            wins += 1
    
    print(f"è®­ç»ƒåèƒœç‡: {wins/test_games:.2%}")
```

## ğŸ¤– 6. å¤§è¯­è¨€æ¨¡å‹AIï¼ˆåˆ›æ–°é€‰åšï¼‰

### åŸºæœ¬æ€è·¯
- å°†æ¸¸æˆçŠ¶æ€è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°
- ä½¿ç”¨æç¤ºå·¥ç¨‹æŒ‡å¯¼å¤§æ¨¡å‹åšå†³ç­–
- å¯ä»¥ä½¿ç”¨APIæˆ–æœ¬åœ°æ¨¡å‹

### åŸºç¡€å®ç°æ¡†æ¶

```python
import json
import requests
from typing import Optional

class LLMBot(BaseAgent):
    def __init__(self, name="LLMBot", player_id=1, model_type="openai"):
        super().__init__(name, player_id)
        self.model_type = model_type
        self.api_key = None  # è®¾ç½®ä½ çš„APIå¯†é’¥
        
    def get_action(self, observation, env):
        try:
            # è½¬æ¢æ¸¸æˆçŠ¶æ€ä¸ºæ–‡å­—æè¿°
            game_description = self.observation_to_text(observation, env)
            
            # æ„å»ºæç¤ºè¯
            prompt = self.build_prompt(game_description, env)
            
            # è°ƒç”¨å¤§æ¨¡å‹
            response = self.call_llm(prompt)
            
            # è§£æå›å¤è·å–åŠ¨ä½œ
            action = self.parse_action(response, env)
            
            if action and action in env.get_valid_actions():
                return action
            else:
                # å¦‚æœè§£æå¤±è´¥ï¼Œé™çº§åˆ°è§„åˆ™ç­–ç•¥
                return self.fallback_strategy(observation, env)
                
        except Exception as e:
            print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return self.fallback_strategy(observation, env)
    
    def observation_to_text(self, observation, env):
        """å°†æ¸¸æˆçŠ¶æ€è½¬æ¢ä¸ºæ–‡å­—æè¿°"""
        if hasattr(env, 'board_size'):  # äº”å­æ£‹
            board = observation['board']
            description = f"æ£‹ç›˜å¤§å°: {board.shape[0]}x{board.shape[1]}\n"
            
            # æè¿°æ£‹ç›˜çŠ¶æ€
            description += "å½“å‰æ£‹ç›˜çŠ¶æ€:\n"
            for i in range(board.shape[0]):
                row_desc = ""
                for j in range(board.shape[1]):
                    if board[i, j] == 0:
                        row_desc += "Â·"
                    elif board[i, j] == 1:
                        row_desc += "â—"
                    else:
                        row_desc += "â—‹"
                description += f"{i:2d}|{row_desc}|\n"
            
            # æ·»åŠ åˆ—æ ‡å·
            col_numbers = "  " + "".join([str(i%10) for i in range(board.shape[1])])
            description += col_numbers + "\n"
            
            return description
            
        else:  # è´ªåƒè›‡
            snake1 = observation.get('snake1', [])
            snake2 = observation.get('snake2', [])
            foods = observation.get('foods', [])
            
            description = f"è´ªåƒè›‡æ¸¸æˆçŠ¶æ€:\n"
            description += f"ä½ çš„è›‡(ç©å®¶{self.player_id}): {snake1 if self.player_id == 1 else snake2}\n"
            description += f"å¯¹æ‰‹çš„è›‡: {snake2 if self.player_id == 1 else snake1}\n"
            description += f"é£Ÿç‰©ä½ç½®: {foods}\n"
            
            return description
    
    def build_prompt(self, game_description, env):
        """æ„å»ºç»™å¤§æ¨¡å‹çš„æç¤ºè¯"""
        game_name = env.__class__.__name__.replace('Env', '')
        valid_actions = env.get_valid_actions()
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{game_name}æ¸¸æˆAIã€‚

{game_description}

ä½ æ˜¯ç©å®¶{self.player_id}ã€‚è¯·åˆ†æå½“å‰å±€åŠ¿å¹¶é€‰æ‹©æœ€ä½³åŠ¨ä½œã€‚

å¯é€‰åŠ¨ä½œ: {valid_actions}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›å¤:
åˆ†æ: [ä½ çš„åˆ†æ]
åŠ¨ä½œ: (row, col)

æ³¨æ„:
1. åªèƒ½ä»å¯é€‰åŠ¨ä½œä¸­é€‰æ‹©
2. åŠ¨ä½œæ ¼å¼å¿…é¡»æ˜¯(row, col)çš„å½¢å¼
3. ä¼˜å…ˆè€ƒè™‘è·èƒœæœºä¼š
4. å…¶æ¬¡è€ƒè™‘é˜»æ­¢å¯¹æ‰‹è·èƒœ
5. é€‰æ‹©æˆ˜ç•¥ä½ç½®
"""
        return prompt
    
    def call_llm(self, prompt):
        """è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹"""
        if self.model_type == "openai":
            return self.call_openai(prompt)
        elif self.model_type == "ollama":
            return self.call_ollama(prompt)
        else:
            # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨è§„åˆ™æ¨¡æ‹Ÿå¤§æ¨¡å‹å›å¤
            return self.simulate_llm_response(prompt)
    
    def call_openai(self, prompt):
        """è°ƒç”¨OpenAI API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7,
            "max_tokens": 200
        }
        
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=10
        )
        
        return response.json()["choices"][0]["message"]["content"]
    
    def call_ollama(self, prompt):
        """è°ƒç”¨æœ¬åœ°Ollamaæ¨¡å‹"""
        data = {
            "model": "llama2",  # æˆ–å…¶ä»–æ¨¡å‹
            "prompt": prompt,
            "stream": False
        }
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=data,
            timeout=30
        )
        
        return response.json()["response"]
    
    def simulate_llm_response(self, prompt):
        """æ¨¡æ‹Ÿå¤§æ¨¡å‹å›å¤ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°ä¸€ä¸ªç®€å•çš„è§„åˆ™æ¥æ¨¡æ‹Ÿå¤§æ¨¡å‹çš„å›å¤
        return "åˆ†æ: é€‰æ‹©ä¸­å¿ƒä½ç½®\nåŠ¨ä½œ: (4, 4)"
    
    def parse_action(self, response, env):
        """ä»å¤§æ¨¡å‹å›å¤ä¸­è§£æåŠ¨ä½œ"""
        import re
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–åŠ¨ä½œ
        pattern = r'åŠ¨ä½œ[:ï¼š]\s*\((\d+),\s*(\d+)\)'
        match = re.search(pattern, response)
        
        if match:
            row, col = int(match.group(1)), int(match.group(2))
            return (row, col)
        
        # å°è¯•å…¶ä»–æ ¼å¼
        pattern2 = r'\((\d+),\s*(\d+)\)'
        matches = re.findall(pattern2, response)
        if matches:
            row, col = int(matches[-1][0]), int(matches[-1][1])
            return (row, col)
        
        return None
    
    def fallback_strategy(self, observation, env):
        """é™çº§ç­–ç•¥"""
        # å¦‚æœLLMå¤±è´¥ï¼Œä½¿ç”¨ç®€å•è§„åˆ™
        valid_actions = env.get_valid_actions()
        if valid_actions:
            # ä¼˜å…ˆé€‰æ‹©ä¸­å¿ƒä½ç½®
            if hasattr(env, 'board_size'):
                center = env.board_size // 2
                for action in valid_actions:
                    row, col = action
                    if abs(row - center) <= 1 and abs(col - center) <= 1:
                        return action
            return random.choice(valid_actions)
        return None
```

### ä½¿ç”¨ç¤ºä¾‹

```python
def test_llm_bot():
    """æµ‹è¯•LLM Bot"""
    from games.gomoku import GomokuEnv
    
    # åˆ›å»ºç¯å¢ƒå’ŒAI
    env = GomokuEnv(board_size=9, win_length=5)
    
    # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ— éœ€APIå¯†é’¥ï¼‰
    llm_bot = LLMBot("LLM Bot", 1, model_type="simulate")
    
    # å¦‚æœè¦ä½¿ç”¨çœŸå®APIï¼Œéœ€è¦è®¾ç½®å¯†é’¥
    # llm_bot.api_key = "your-openai-api-key"
    # llm_bot.model_type = "openai"
    
    observation, _ = env.reset()
    action = llm_bot.get_action(observation, env)
    print(f"LLMé€‰æ‹©çš„åŠ¨ä½œ: {action}")
```

### ä½¿ç”¨å»ºè®®

1. **APIè´¹ç”¨æ§åˆ¶**: è®¾ç½®è¯·æ±‚é™åˆ¶ï¼Œé¿å…è¿‡åº¦è°ƒç”¨
2. **æç¤ºè¯ä¼˜åŒ–**: å¤šæµ‹è¯•ä¸åŒçš„æç¤ºè¯æ ¼å¼
3. **é”™è¯¯å¤„ç†**: å¿…é¡»æœ‰é™çº§ç­–ç•¥
4. **æœ¬åœ°æ¨¡å‹**: å¯ä»¥ä½¿ç”¨Ollamaç­‰å…è´¹æœ¬åœ°æ¨¡å‹

## ğŸ“ˆ è¿›é˜¶æ–¹å‘

å®ŒæˆåŸºç¡€AIåï¼Œå¯ä»¥å°è¯•ï¼š

1. **ç»„åˆç­–ç•¥**: ç»“åˆå¤šç§ç®—æ³•çš„ä¼˜åŠ¿
2. **è‡ªé€‚åº”AI**: æ ¹æ®å¯¹æ‰‹é£æ ¼è°ƒæ•´ç­–ç•¥
3. **å¼ºåŒ–å­¦ä¹ **: Q-learningã€Actor-Criticç­‰
4. **ç¥ç»ç½‘ç»œ**: æ·±åº¦å­¦ä¹ æ–¹æ³•
5. **å¤šæ™ºèƒ½ä½“å­¦ä¹ **: æ™ºèƒ½ä½“ä¹‹é—´çš„åä½œä¸ç«äº‰

è®°ä½ï¼šå¥½çš„AIä¸æ˜¯æœ€å¤æ‚çš„ç®—æ³•ï¼Œè€Œæ˜¯æœ€é€‚åˆç‰¹å®šæ¸¸æˆçš„ç­–ç•¥ï¼ 